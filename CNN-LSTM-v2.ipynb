{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s25 = np.load('s25.npy')\n",
    "h25 = np.load('h25.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s25_labels = np.array([0 for _ in range(0,len(s25))])\n",
    "h25_labels = np.array([1 for _ in range(0,len(h25))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(s25,h25,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.append(s25_labels,h25_labels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1142, 6250, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 1, 50, 128\n",
    "    n_features = trainX.shape[2]\n",
    "    n_steps, n_length = 10, 625\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(50, activation= 'relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01) ))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation= 'sigmoid' ))\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=['accuracy',Precision(),Recall(),AUC()])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy,precision,recall,auc = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy,precision,recall,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(accuracies,precisions,recalls,aucs):\n",
    "    m, s = mean(accuracies), std(accuracies)\n",
    "    print( ' Accuracy: %.3f%% (+/-%.3f) ' % (m, s))\n",
    "    m, s = mean(precisions), std(precisions)\n",
    "    print( ' Precision: %.3f%% (+/-%.3f) ' % (m, s))\n",
    "    m, s = mean(recalls), std(recalls)\n",
    "    print( ' Recall: %.3f%% (+/-%.3f) ' % (m, s))\n",
    "    m, s = mean(aucs), std(aucs)\n",
    "    print( ' AUC: %.3f%% (+/-%.3f) ' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X,y,repeats=5):\n",
    "    # load data\n",
    "    trainX, testX,trainy, testy = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    accuracies = list()\n",
    "    precisions = list()\n",
    "    recalls = list()\n",
    "    aucs = list()\n",
    "    for r in range(repeats):\n",
    "        accuracy,precision,recall,auc = evaluate_model(trainX, trainy, testX, testy)\n",
    "        accuracy = accuracy * 100.0\n",
    "        precision = precision * 100.0\n",
    "        recall = recall * 100.0\n",
    "        auc = auc * 100.0\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        aucs.append(auc)\n",
    "    # summarize results\n",
    "    summarize_results(accuracies,precisions,recalls,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 1.3680 - accuracy: 0.5148 - precision: 0.4789 - recall: 0.3972 - auc: 0.5080\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.3082 - accuracy: 0.4984 - precision: 0.4698 - recall: 0.5444 - auc: 0.5035\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.2415 - accuracy: 0.5257 - precision: 0.4937 - recall: 0.4556 - auc: 0.5248\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 1.1979 - accuracy: 0.5093 - precision: 0.4730 - recall: 0.4089 - auc: 0.5112\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 1.1618 - accuracy: 0.4786 - precision: 0.4341 - recall: 0.3692 - auc: 0.4766\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 1.1030 - accuracy: 0.5356 - precision: 0.5080 - recall: 0.2967 - auc: 0.5448\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 1.0856 - accuracy: 0.5214 - precision: 0.4651 - recall: 0.1402 - auc: 0.4792\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.0417 - accuracy: 0.5257 - precision: 0.4847 - recall: 0.1846 - auc: 0.5067\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.0118 - accuracy: 0.5049 - precision: 0.4603 - recall: 0.3248 - auc: 0.4911\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.9795 - accuracy: 0.5148 - precision: 0.4820 - recall: 0.4696 - auc: 0.5140\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 0.9490 - accuracy: 0.5476 - precision: 0.5193 - recall: 0.4720 - auc: 0.5419\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.9345 - accuracy: 0.4907 - precision: 0.4401 - recall: 0.3178 - auc: 0.4868\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.9063 - accuracy: 0.5422 - precision: 0.5234 - recall: 0.2617 - auc: 0.5353\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.8896 - accuracy: 0.5257 - precision: 0.4872 - recall: 0.2220 - auc: 0.5133\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.8773 - accuracy: 0.5192 - precision: 0.4774 - recall: 0.2710 - auc: 0.4953\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.8528 - accuracy: 0.5433 - precision: 0.5268 - recall: 0.2523 - auc: 0.5429\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.8378 - accuracy: 0.5498 - precision: 0.5556 - recall: 0.1986 - auc: 0.5494\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.8345 - accuracy: 0.5367 - precision: 0.5170 - recall: 0.1776 - auc: 0.4862\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.8182 - accuracy: 0.5356 - precision: 0.5079 - recall: 0.3014 - auc: 0.5195\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.8145 - accuracy: 0.4830 - precision: 0.4453 - recall: 0.4182 - auc: 0.4716\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.8048 - accuracy: 0.4819 - precision: 0.4344 - recall: 0.3481 - auc: 0.4609\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.7918 - accuracy: 0.5049 - precision: 0.4464 - recall: 0.2336 - auc: 0.4803\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7830 - accuracy: 0.5268 - precision: 0.4882 - recall: 0.1939 - auc: 0.4914\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7759 - accuracy: 0.5312 - precision: 0.5000 - recall: 0.1379 - auc: 0.4979\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7673 - accuracy: 0.5378 - precision: 0.5250 - recall: 0.1472 - auc: 0.5165\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7624 - accuracy: 0.5235 - precision: 0.4759 - recall: 0.1612 - auc: 0.5008\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.7554 - accuracy: 0.5290 - precision: 0.4931 - recall: 0.1659 - auc: 0.5210\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.7521 - accuracy: 0.5192 - precision: 0.4615 - recall: 0.1542 - auc: 0.5049\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7486 - accuracy: 0.5279 - precision: 0.4925 - recall: 0.2290 - auc: 0.4956\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7449 - accuracy: 0.5279 - precision: 0.4930 - recall: 0.2453 - auc: 0.4874\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7416 - accuracy: 0.4995 - precision: 0.4190 - recall: 0.1752 - auc: 0.4761\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.7364 - accuracy: 0.5268 - precision: 0.4787 - recall: 0.1051 - auc: 0.4975\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7320 - accuracy: 0.5301 - precision: 0.4915 - recall: 0.0678 - auc: 0.5199\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7282 - accuracy: 0.5433 - precision: 0.6897 - recall: 0.0467 - auc: 0.5297\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7262 - accuracy: 0.5389 - precision: 0.6129 - recall: 0.0444 - auc: 0.5217\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7261 - accuracy: 0.5181 - precision: 0.4118 - recall: 0.0654 - auc: 0.4884\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.7243 - accuracy: 0.5225 - precision: 0.4474 - recall: 0.0794 - auc: 0.4906\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7207 - accuracy: 0.5345 - precision: 0.5118 - recall: 0.1519 - auc: 0.5023\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7174 - accuracy: 0.5323 - precision: 0.5030 - recall: 0.1963 - auc: 0.5338\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7182 - accuracy: 0.5290 - precision: 0.4857 - recall: 0.0794 - auc: 0.4990\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7137 - accuracy: 0.5279 - precision: 0.4545 - recall: 0.0350 - auc: 0.5447\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7154 - accuracy: 0.5235 - precision: 0.3793 - recall: 0.0257 - auc: 0.5018\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7147 - accuracy: 0.5323 - precision: 0.5070 - recall: 0.0841 - auc: 0.4887\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.7132 - accuracy: 0.5235 - precision: 0.4578 - recall: 0.0888 - auc: 0.4936\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7138 - accuracy: 0.5279 - precision: 0.4667 - recall: 0.0491 - auc: 0.4691\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.7097 - accuracy: 0.5257 - precision: 0.4359 - recall: 0.0397 - auc: 0.5088\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.7097 - accuracy: 0.5323 - precision: 0.5143 - recall: 0.0421 - auc: 0.4851\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.7094 - accuracy: 0.5290 - precision: 0.4615 - recall: 0.0280 - auc: 0.4737\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.7062 - accuracy: 0.5334 - precision: 0.5556 - recall: 0.0234 - auc: 0.5074\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.7068 - accuracy: 0.5257 - precision: 0.3529 - recall: 0.0140 - auc: 0.4907\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.3648 - accuracy: 0.4655 - precision_1: 0.4296 - recall_1: 0.4276 - auc_1: 0.4837\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.3019 - accuracy: 0.5005 - precision_1: 0.4493 - recall_1: 0.2897 - auc_1: 0.5018\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.2461 - accuracy: 0.5159 - precision_1: 0.4768 - recall_1: 0.3364 - auc_1: 0.4989\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 1.1968 - accuracy: 0.5170 - precision_1: 0.4743 - recall_1: 0.2804 - auc_1: 0.5077\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 58ms/step - loss: 1.1549 - accuracy: 0.5093 - precision_1: 0.4688 - recall_1: 0.3505 - auc_1: 0.4935\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.1142 - accuracy: 0.5071 - precision_1: 0.4728 - recall_1: 0.4463 - auc_1: 0.5040\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0739 - accuracy: 0.5235 - precision_1: 0.4901 - recall_1: 0.4042 - auc_1: 0.5156\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.0484 - accuracy: 0.5148 - precision_1: 0.4675 - recall_1: 0.2523 - auc_1: 0.4845\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.0143 - accuracy: 0.4995 - precision_1: 0.4393 - recall_1: 0.2453 - auc_1: 0.4919\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9804 - accuracy: 0.5159 - precision_1: 0.4778 - recall_1: 0.3528 - auc_1: 0.5269\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9639 - accuracy: 0.4907 - precision_1: 0.4245 - recall_1: 0.2430 - auc_1: 0.4753\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.9345 - accuracy: 0.5367 - precision_1: 0.5148 - recall_1: 0.2033 - auc_1: 0.5097\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.9188 - accuracy: 0.5038 - precision_1: 0.4352 - recall_1: 0.1963 - auc_1: 0.4741\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.8923 - accuracy: 0.5181 - precision_1: 0.4720 - recall_1: 0.2360 - auc_1: 0.5174\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.8803 - accuracy: 0.5192 - precision_1: 0.4760 - recall_1: 0.2547 - auc_1: 0.4837\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.8673 - accuracy: 0.4765 - precision_1: 0.4023 - recall_1: 0.2407 - auc_1: 0.4641\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.8491 - accuracy: 0.5104 - precision_1: 0.4644 - recall_1: 0.2897 - auc_1: 0.4979\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.8372 - accuracy: 0.5016 - precision_1: 0.4498 - recall_1: 0.2827 - auc_1: 0.4756\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.8237 - accuracy: 0.5225 - precision_1: 0.4789 - recall_1: 0.2126 - auc_1: 0.4911\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.8168 - accuracy: 0.4863 - precision_1: 0.3624 - recall_1: 0.1262 - auc_1: 0.4579\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.8039 - accuracy: 0.5071 - precision_1: 0.4304 - recall_1: 0.1589 - auc_1: 0.4729\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7912 - accuracy: 0.5290 - precision_1: 0.4932 - recall_1: 0.1682 - auc_1: 0.5137\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.7871 - accuracy: 0.5126 - precision_1: 0.4479 - recall_1: 0.1706 - auc_1: 0.4637\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7755 - accuracy: 0.5290 - precision_1: 0.4925 - recall_1: 0.1542 - auc_1: 0.5193\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7696 - accuracy: 0.5126 - precision_1: 0.4388 - recall_1: 0.1425 - auc_1: 0.5044\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7664 - accuracy: 0.5268 - precision_1: 0.4811 - recall_1: 0.1192 - auc_1: 0.4737\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.7590 - accuracy: 0.5192 - precision_1: 0.4409 - recall_1: 0.0958 - auc_1: 0.4889\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.7525 - accuracy: 0.5323 - precision_1: 0.5068 - recall_1: 0.0864 - auc_1: 0.5186\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.7499 - accuracy: 0.5115 - precision_1: 0.3902 - recall_1: 0.0748 - auc_1: 0.4875\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7446 - accuracy: 0.5290 - precision_1: 0.4943 - recall_1: 0.2033 - auc_1: 0.5036\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7443 - accuracy: 0.4896 - precision_1: 0.4291 - recall_1: 0.2687 - auc_1: 0.4600\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7388 - accuracy: 0.5323 - precision_1: 0.5031 - recall_1: 0.1869 - auc_1: 0.4836\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7344 - accuracy: 0.5126 - precision_1: 0.4045 - recall_1: 0.0841 - auc_1: 0.4901\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.7294 - accuracy: 0.5279 - precision_1: 0.4694 - recall_1: 0.0537 - auc_1: 0.5163\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7316 - accuracy: 0.5301 - precision_1: 0.4889 - recall_1: 0.0514 - auc_1: 0.4666\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7260 - accuracy: 0.5159 - precision_1: 0.3056 - recall_1: 0.0257 - auc_1: 0.5004\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7242 - accuracy: 0.5214 - precision_1: 0.4366 - recall_1: 0.0724 - auc_1: 0.4895\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7215 - accuracy: 0.5159 - precision_1: 0.4255 - recall_1: 0.0935 - auc_1: 0.5000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7191 - accuracy: 0.5279 - precision_1: 0.4860 - recall_1: 0.1215 - auc_1: 0.4901\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7177 - accuracy: 0.5181 - precision_1: 0.4333 - recall_1: 0.0911 - auc_1: 0.4949\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.7155 - accuracy: 0.5290 - precision_1: 0.4861 - recall_1: 0.0818 - auc_1: 0.5057\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.7156 - accuracy: 0.5268 - precision_1: 0.4831 - recall_1: 0.1332 - auc_1: 0.4865\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7138 - accuracy: 0.5159 - precision_1: 0.4426 - recall_1: 0.1262 - auc_1: 0.4912\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.7137 - accuracy: 0.5115 - precision_1: 0.4505 - recall_1: 0.1916 - auc_1: 0.4719\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7097 - accuracy: 0.5290 - precision_1: 0.4950 - recall_1: 0.2313 - auc_1: 0.5177\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.7072 - accuracy: 0.5378 - precision_1: 0.5405 - recall_1: 0.0935 - auc_1: 0.5332\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7098 - accuracy: 0.5301 - precision_1: 0.4706 - recall_1: 0.0187 - auc_1: 0.4910\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.7080 - accuracy: 0.5192 - precision_1: 0.3922 - recall_1: 0.0467 - auc_1: 0.4964\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.7061 - accuracy: 0.5345 - precision_1: 0.5084 - recall_1: 0.2126 - auc_1: 0.5115\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7059 - accuracy: 0.5159 - precision_1: 0.4578 - recall_1: 0.1776 - auc_1: 0.4909\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.3491 - accuracy: 0.5170 - precision_2: 0.4815 - recall_2: 0.3949 - auc_2: 0.5013\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.2951 - accuracy: 0.4852 - precision_2: 0.4559 - recall_2: 0.5070 - auc_2: 0.4914\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.2416 - accuracy: 0.5170 - precision_2: 0.4821 - recall_2: 0.4089 - auc_2: 0.5007\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.1860 - accuracy: 0.5192 - precision_2: 0.4831 - recall_2: 0.3668 - auc_2: 0.5234\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 1.1532 - accuracy: 0.5290 - precision_2: 0.4965 - recall_2: 0.3318 - auc_2: 0.4884\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.1066 - accuracy: 0.5060 - precision_2: 0.4650 - recall_2: 0.3575 - auc_2: 0.5007\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.0695 - accuracy: 0.5378 - precision_2: 0.5081 - recall_2: 0.4393 - auc_2: 0.5183\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 63ms/step - loss: 1.0397 - accuracy: 0.5060 - precision_2: 0.4661 - recall_2: 0.3692 - auc_2: 0.4947\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.0084 - accuracy: 0.5082 - precision_2: 0.4658 - recall_2: 0.3341 - auc_2: 0.4934\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.9806 - accuracy: 0.4973 - precision_2: 0.4424 - recall_2: 0.2780 - auc_2: 0.4955\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.9568 - accuracy: 0.4995 - precision_2: 0.4436 - recall_2: 0.2664 - auc_2: 0.4875\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.9338 - accuracy: 0.4918 - precision_2: 0.4437 - recall_2: 0.3318 - auc_2: 0.4840\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.9084 - accuracy: 0.5159 - precision_2: 0.4770 - recall_2: 0.3388 - auc_2: 0.5138\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.8873 - accuracy: 0.5301 - precision_2: 0.4983 - recall_2: 0.3388 - auc_2: 0.5414\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.8739 - accuracy: 0.5214 - precision_2: 0.4851 - recall_2: 0.3435 - auc_2: 0.5181\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.8572 - accuracy: 0.5301 - precision_2: 0.4987 - recall_2: 0.4533 - auc_2: 0.5290\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.8450 - accuracy: 0.5082 - precision_2: 0.4697 - recall_2: 0.3808 - auc_2: 0.5110\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.8392 - accuracy: 0.4962 - precision_2: 0.4420 - recall_2: 0.2850 - auc_2: 0.4614\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.8247 - accuracy: 0.5093 - precision_2: 0.4367 - recall_2: 0.1612 - auc_2: 0.4827\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.8093 - accuracy: 0.5389 - precision_2: 0.5310 - recall_2: 0.1402 - auc_2: 0.5191\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.8046 - accuracy: 0.5148 - precision_2: 0.4157 - recall_2: 0.0864 - auc_2: 0.4860\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7923 - accuracy: 0.5170 - precision_2: 0.4558 - recall_2: 0.1565 - auc_2: 0.5059\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.7875 - accuracy: 0.4995 - precision_2: 0.4233 - recall_2: 0.1869 - auc_2: 0.4783\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7751 - accuracy: 0.5301 - precision_2: 0.4969 - recall_2: 0.1846 - auc_2: 0.5167\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.7753 - accuracy: 0.5115 - precision_2: 0.4274 - recall_2: 0.1238 - auc_2: 0.4527\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.7655 - accuracy: 0.5214 - precision_2: 0.4554 - recall_2: 0.1075 - auc_2: 0.4906\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7604 - accuracy: 0.5060 - precision_2: 0.4320 - recall_2: 0.1706 - auc_2: 0.4841\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7578 - accuracy: 0.5126 - precision_2: 0.4465 - recall_2: 0.1659 - auc_2: 0.4579\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.7489 - accuracy: 0.5279 - precision_2: 0.4916 - recall_2: 0.2056 - auc_2: 0.5069\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.7448 - accuracy: 0.5093 - precision_2: 0.4515 - recall_2: 0.2173 - auc_2: 0.5059\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7428 - accuracy: 0.4984 - precision_2: 0.4257 - recall_2: 0.2009 - auc_2: 0.4760\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7404 - accuracy: 0.5137 - precision_2: 0.4429 - recall_2: 0.1449 - auc_2: 0.4705\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7370 - accuracy: 0.5159 - precision_2: 0.4568 - recall_2: 0.1729 - auc_2: 0.4759\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7325 - accuracy: 0.5192 - precision_2: 0.4654 - recall_2: 0.1729 - auc_2: 0.4815\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7296 - accuracy: 0.5225 - precision_2: 0.4565 - recall_2: 0.0981 - auc_2: 0.4863\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.7257 - accuracy: 0.5181 - precision_2: 0.3636 - recall_2: 0.0374 - auc_2: 0.5013\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7241 - accuracy: 0.5323 - precision_2: 0.5333 - recall_2: 0.0187 - auc_2: 0.4972\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.7206 - accuracy: 0.5323 - precision_2: 0.5455 - recall_2: 0.0140 - auc_2: 0.5215\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7199 - accuracy: 0.5225 - precision_2: 0.3824 - recall_2: 0.0304 - auc_2: 0.5085\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7174 - accuracy: 0.5433 - precision_2: 0.5733 - recall_2: 0.1005 - auc_2: 0.5046\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7179 - accuracy: 0.5148 - precision_2: 0.4400 - recall_2: 0.1285 - auc_2: 0.4867\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7136 - accuracy: 0.5334 - precision_2: 0.5083 - recall_2: 0.1425 - auc_2: 0.5250\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7150 - accuracy: 0.5159 - precision_2: 0.4239 - recall_2: 0.0911 - auc_2: 0.4680\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7120 - accuracy: 0.5290 - precision_2: 0.4722 - recall_2: 0.0397 - auc_2: 0.4955\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7113 - accuracy: 0.5290 - precision_2: 0.4375 - recall_2: 0.0164 - auc_2: 0.4937\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7095 - accuracy: 0.5268 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - auc_2: 0.5066\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7094 - accuracy: 0.5301 - precision_2: 0.4000 - recall_2: 0.0047 - auc_2: 0.4792\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7069 - accuracy: 0.5279 - precision_2: 0.3846 - recall_2: 0.0117 - auc_2: 0.4991\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.7064 - accuracy: 0.5345 - precision_2: 0.6000 - recall_2: 0.0210 - auc_2: 0.4989\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.7042 - accuracy: 0.5367 - precision_2: 0.6923 - recall_2: 0.0210 - auc_2: 0.5200\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.3636 - accuracy: 0.4819 - precision_3: 0.4427 - recall_3: 0.4065 - auc_3: 0.4849\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 1.2999 - accuracy: 0.5027 - precision_3: 0.4454 - recall_3: 0.2477 - auc_3: 0.4874\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.2400 - accuracy: 0.5082 - precision_3: 0.4675 - recall_3: 0.3528 - auc_3: 0.5048\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 1.1920 - accuracy: 0.5148 - precision_3: 0.4802 - recall_3: 0.4252 - auc_3: 0.4967\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.1523 - accuracy: 0.5016 - precision_3: 0.4539 - recall_3: 0.3107 - auc_3: 0.4753\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.1061 - accuracy: 0.5049 - precision_3: 0.4595 - recall_3: 0.3178 - auc_3: 0.4885\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.0685 - accuracy: 0.4962 - precision_3: 0.4532 - recall_3: 0.3621 - auc_3: 0.4853\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 1.0308 - accuracy: 0.5104 - precision_3: 0.4706 - recall_3: 0.3551 - auc_3: 0.5047: 0s - loss: 1.0308 - accuracy: 0.5112 - precision_3: 0.4698 - recall_3: 0.3532 - auc_3: 0.506\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.0025 - accuracy: 0.4907 - precision_3: 0.4327 - recall_3: 0.2780 - auc_3: 0.4840\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 70ms/step - loss: 0.9698 - accuracy: 0.5345 - precision_3: 0.5078 - recall_3: 0.2290 - auc_3: 0.5093\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.9442 - accuracy: 0.5203 - precision_3: 0.4691 - recall_3: 0.1776 - auc_3: 0.5026\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.9207 - accuracy: 0.5279 - precision_3: 0.4914 - recall_3: 0.2009 - auc_3: 0.5041\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.9048 - accuracy: 0.5049 - precision_3: 0.4259 - recall_3: 0.1612 - auc_3: 0.4719\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.8835 - accuracy: 0.5137 - precision_3: 0.4429 - recall_3: 0.1449 - auc_3: 0.4924: 0s - loss: 0.8858 - accuracy: 0.5052 - precision_3: 0.4390 - recall_3: 0.1479 - auc_3: 0.49\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.8639 - accuracy: 0.5170 - precision_3: 0.4660 - recall_3: 0.2079 - auc_3: 0.4950\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.8515 - accuracy: 0.4885 - precision_3: 0.4270 - recall_3: 0.2664 - auc_3: 0.4697\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.8343 - accuracy: 0.5214 - precision_3: 0.4789 - recall_3: 0.2383 - auc_3: 0.5029\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.8189 - accuracy: 0.5356 - precision_3: 0.5097 - recall_3: 0.2453 - auc_3: 0.5257\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.8100 - accuracy: 0.5071 - precision_3: 0.4513 - recall_3: 0.2383 - auc_3: 0.4939\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.8000 - accuracy: 0.5159 - precision_3: 0.4573 - recall_3: 0.1752 - auc_3: 0.4854\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7918 - accuracy: 0.5170 - precision_3: 0.4545 - recall_3: 0.1519 - auc_3: 0.4732\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7819 - accuracy: 0.5257 - precision_3: 0.4790 - recall_3: 0.1332 - auc_3: 0.4922\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7725 - accuracy: 0.5170 - precision_3: 0.4235 - recall_3: 0.0841 - auc_3: 0.5133\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7664 - accuracy: 0.5257 - precision_3: 0.4444 - recall_3: 0.0467 - auc_3: 0.5143\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.7606 - accuracy: 0.5093 - precision_3: 0.3529 - recall_3: 0.0561 - auc_3: 0.5018\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7546 - accuracy: 0.5246 - precision_3: 0.4583 - recall_3: 0.0771 - auc_3: 0.5010\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7516 - accuracy: 0.5203 - precision_3: 0.4265 - recall_3: 0.0678 - auc_3: 0.4729\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.7446 - accuracy: 0.5290 - precision_3: 0.4875 - recall_3: 0.0911 - auc_3: 0.4935\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.7416 - accuracy: 0.5312 - precision_3: 0.5000 - recall_3: 0.0818 - auc_3: 0.4778\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7364 - accuracy: 0.5279 - precision_3: 0.4694 - recall_3: 0.0537 - auc_3: 0.4914\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.7316 - accuracy: 0.5323 - precision_3: 0.5102 - recall_3: 0.0584 - auc_3: 0.5074\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.7305 - accuracy: 0.5312 - precision_3: 0.5000 - recall_3: 0.1075 - auc_3: 0.4910\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7274 - accuracy: 0.5126 - precision_3: 0.3976 - recall_3: 0.0771 - auc_3: 0.4907\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.7246 - accuracy: 0.5301 - precision_3: 0.4955 - recall_3: 0.1285 - auc_3: 0.4974\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.7227 - accuracy: 0.5279 - precision_3: 0.4887 - recall_3: 0.1519 - auc_3: 0.5030\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7204 - accuracy: 0.5214 - precision_3: 0.4554 - recall_3: 0.1075 - auc_3: 0.4908\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7188 - accuracy: 0.5301 - precision_3: 0.4935 - recall_3: 0.0888 - auc_3: 0.4938\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7164 - accuracy: 0.5246 - precision_3: 0.4615 - recall_3: 0.0841 - auc_3: 0.5054\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.7140 - accuracy: 0.5356 - precision_3: 0.5278 - recall_3: 0.0888 - auc_3: 0.5027\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7146 - accuracy: 0.5257 - precision_3: 0.4419 - recall_3: 0.0444 - auc_3: 0.4725\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.7097 - accuracy: 0.5378 - precision_3: 0.5577 - recall_3: 0.0678 - auc_3: 0.5213\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7097 - accuracy: 0.5367 - precision_3: 0.5714 - recall_3: 0.0467 - auc_3: 0.5027\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7084 - accuracy: 0.5279 - precision_3: 0.4545 - recall_3: 0.0350 - auc_3: 0.5134\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7071 - accuracy: 0.5235 - precision_3: 0.4426 - recall_3: 0.0631 - auc_3: 0.5182\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.7086 - accuracy: 0.5104 - precision_3: 0.3882 - recall_3: 0.0771 - auc_3: 0.4657\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7054 - accuracy: 0.5312 - precision_3: 0.5000 - recall_3: 0.0771 - auc_3: 0.5050\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7055 - accuracy: 0.5203 - precision_3: 0.4375 - recall_3: 0.0818 - auc_3: 0.4954\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7057 - accuracy: 0.5268 - precision_3: 0.4565 - recall_3: 0.0491 - auc_3: 0.4667\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.7046 - accuracy: 0.5268 - precision_3: 0.4375 - recall_3: 0.0327 - auc_3: 0.4760\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7032 - accuracy: 0.5279 - precision_3: 0.3846 - recall_3: 0.0117 - auc_3: 0.4882\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.3539 - accuracy: 0.5181 - precision_4: 0.4784 - recall_4: 0.3107 - auc_4: 0.5102\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.2913 - accuracy: 0.5148 - precision_4: 0.4766 - recall_4: 0.3575 - auc_4: 0.5075\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.2451 - accuracy: 0.4962 - precision_4: 0.4467 - recall_4: 0.3131 - auc_4: 0.4827\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.1884 - accuracy: 0.5148 - precision_4: 0.4802 - recall_4: 0.4252 - auc_4: 0.5173\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 1.1490 - accuracy: 0.4929 - precision_4: 0.4484 - recall_4: 0.3551 - auc_4: 0.4814\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.1095 - accuracy: 0.5334 - precision_4: 0.5040 - recall_4: 0.2921 - auc_4: 0.4821\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0684 - accuracy: 0.5214 - precision_4: 0.4681 - recall_4: 0.1542 - auc_4: 0.5051\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.0370 - accuracy: 0.5214 - precision_4: 0.4694 - recall_4: 0.1612 - auc_4: 0.4840\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9993 - accuracy: 0.5214 - precision_4: 0.4796 - recall_4: 0.2477 - auc_4: 0.5138\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.9775 - accuracy: 0.4907 - precision_4: 0.4147 - recall_4: 0.2103 - auc_4: 0.4680\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.9506 - accuracy: 0.4973 - precision_4: 0.4346 - recall_4: 0.2407 - auc_4: 0.4850\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 58ms/step - loss: 0.9270 - accuracy: 0.5027 - precision_4: 0.4430 - recall_4: 0.2360 - auc_4: 0.4917\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.9082 - accuracy: 0.5257 - precision_4: 0.4847 - recall_4: 0.1846 - auc_4: 0.4786\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.8827 - accuracy: 0.5455 - precision_4: 0.5520 - recall_4: 0.1612 - auc_4: 0.5149\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.8645 - accuracy: 0.5301 - precision_4: 0.4952 - recall_4: 0.1215 - auc_4: 0.5363\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.8524 - accuracy: 0.5104 - precision_4: 0.4596 - recall_4: 0.2523 - auc_4: 0.5001\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.8367 - accuracy: 0.5181 - precision_4: 0.4793 - recall_4: 0.3248 - auc_4: 0.5227\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.8244 - accuracy: 0.5192 - precision_4: 0.4784 - recall_4: 0.2850 - auc_4: 0.5120\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.8127 - accuracy: 0.5345 - precision_4: 0.5066 - recall_4: 0.2710 - auc_4: 0.5258\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.8079 - accuracy: 0.5016 - precision_4: 0.4389 - recall_4: 0.2266 - auc_4: 0.4686\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.7927 - accuracy: 0.5192 - precision_4: 0.4721 - recall_4: 0.2173 - auc_4: 0.5116\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7828 - accuracy: 0.5214 - precision_4: 0.4821 - recall_4: 0.2827 - auc_4: 0.5330\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.7765 - accuracy: 0.5225 - precision_4: 0.4806 - recall_4: 0.2313 - auc_4: 0.5174\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.7722 - accuracy: 0.5170 - precision_4: 0.4558 - recall_4: 0.1565 - auc_4: 0.4773\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7630 - accuracy: 0.5192 - precision_4: 0.4455 - recall_4: 0.1051 - auc_4: 0.5063\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.7574 - accuracy: 0.5389 - precision_4: 0.5660 - recall_4: 0.0701 - auc_4: 0.5092\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7520 - accuracy: 0.5214 - precision_4: 0.3953 - recall_4: 0.0397 - auc_4: 0.5047\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7471 - accuracy: 0.5345 - precision_4: 0.5405 - recall_4: 0.0467 - auc_4: 0.5040\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7430 - accuracy: 0.5323 - precision_4: 0.5094 - recall_4: 0.0631 - auc_4: 0.5082\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7404 - accuracy: 0.5268 - precision_4: 0.4677 - recall_4: 0.0678 - auc_4: 0.5018\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7377 - accuracy: 0.5389 - precision_4: 0.5897 - recall_4: 0.0537 - auc_4: 0.4842\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7310 - accuracy: 0.5137 - precision_4: 0.3710 - recall_4: 0.0537 - auc_4: 0.5257\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.7276 - accuracy: 0.5334 - precision_4: 0.5122 - recall_4: 0.0981 - auc_4: 0.5343\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7278 - accuracy: 0.5323 - precision_4: 0.5056 - recall_4: 0.1051 - auc_4: 0.4933\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7224 - accuracy: 0.5411 - precision_4: 0.5763 - recall_4: 0.0794 - auc_4: 0.5233\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7234 - accuracy: 0.5268 - precision_4: 0.4615 - recall_4: 0.0561 - auc_4: 0.4972\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7197 - accuracy: 0.5268 - precision_4: 0.4333 - recall_4: 0.0304 - auc_4: 0.5088\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7182 - accuracy: 0.5279 - precision_4: 0.4595 - recall_4: 0.0397 - auc_4: 0.5095\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.7152 - accuracy: 0.5235 - precision_4: 0.3793 - recall_4: 0.0257 - auc_4: 0.5302\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7151 - accuracy: 0.5312 - precision_4: 0.5000 - recall_4: 0.0864 - auc_4: 0.5007\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7172 - accuracy: 0.5115 - precision_4: 0.3784 - recall_4: 0.0654 - auc_4: 0.4640\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7120 - accuracy: 0.5148 - precision_4: 0.3913 - recall_4: 0.0631 - auc_4: 0.5092\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7091 - accuracy: 0.5411 - precision_4: 0.5405 - recall_4: 0.1402 - auc_4: 0.5217\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7118 - accuracy: 0.5005 - precision_4: 0.3852 - recall_4: 0.1098 - auc_4: 0.4853\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7074 - accuracy: 0.5301 - precision_4: 0.4950 - recall_4: 0.1168 - auc_4: 0.5249\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.7091 - accuracy: 0.5225 - precision_4: 0.4459 - recall_4: 0.0771 - auc_4: 0.4869\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7065 - accuracy: 0.5268 - precision_4: 0.4231 - recall_4: 0.0257 - auc_4: 0.5199\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7064 - accuracy: 0.5323 - precision_4: 0.5294 - recall_4: 0.0210 - auc_4: 0.4913\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.7056 - accuracy: 0.5323 - precision_4: 0.5333 - recall_4: 0.0187 - auc_4: 0.5046\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.7040 - accuracy: 0.5323 - precision_4: 0.5238 - recall_4: 0.0257 - auc_4: 0.5112\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7124507950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " Accuracy: 61.572% (+/-0.000) \n",
      " Precision: 0.000% (+/-0.000) \n",
      " Recall: 0.000% (+/-0.000) \n",
      " AUC: 49.545% (+/-0.227) \n"
     ]
    }
   ],
   "source": [
    "run_experiment(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.4031 - accuracy: 0.4962 - precision_5: 0.4688 - recall_5: 0.5607 - auc_5: 0.4676\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 1.3110 - accuracy: 0.5203 - precision_5: 0.4871 - recall_5: 0.4416 - auc_5: 0.5185\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 1.2601 - accuracy: 0.5356 - precision_5: 0.5074 - recall_5: 0.3224 - auc_5: 0.5309\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.2221 - accuracy: 0.5126 - precision_5: 0.4745 - recall_5: 0.3692 - auc_5: 0.5065\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 1.1817 - accuracy: 0.5093 - precision_5: 0.4741 - recall_5: 0.4276 - auc_5: 0.5033\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 1.1463 - accuracy: 0.5038 - precision_5: 0.4539 - recall_5: 0.2874 - auc_5: 0.4937\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.1074 - accuracy: 0.5126 - precision_5: 0.4635 - recall_5: 0.2523 - auc_5: 0.5123\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.0739 - accuracy: 0.5148 - precision_5: 0.4766 - recall_5: 0.3575 - auc_5: 0.5140\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.0420 - accuracy: 0.5301 - precision_5: 0.4985 - recall_5: 0.3949 - auc_5: 0.5217\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.0109 - accuracy: 0.5367 - precision_5: 0.5084 - recall_5: 0.3551 - auc_5: 0.5383\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.9874 - accuracy: 0.5389 - precision_5: 0.5145 - recall_5: 0.2897 - auc_5: 0.5326\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.9668 - accuracy: 0.5312 - precision_5: 0.5000 - recall_5: 0.4393 - auc_5: 0.5292\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9533 - accuracy: 0.4852 - precision_5: 0.4595 - recall_5: 0.5561 - auc_5: 0.4996\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.9156 - accuracy: 0.5455 - precision_5: 0.5187 - recall_5: 0.4206 - auc_5: 0.5725\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.9069 - accuracy: 0.5345 - precision_5: 0.5108 - recall_5: 0.1659 - auc_5: 0.5485\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.8947 - accuracy: 0.5389 - precision_5: 0.5191 - recall_5: 0.2220 - auc_5: 0.5212\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.8669 - accuracy: 0.5531 - precision_5: 0.5323 - recall_5: 0.3855 - auc_5: 0.5724\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.8548 - accuracy: 0.5334 - precision_5: 0.5025 - recall_5: 0.4696 - auc_5: 0.5673\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.8409 - accuracy: 0.5345 - precision_5: 0.5040 - recall_5: 0.4416 - auc_5: 0.5675\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.8230 - accuracy: 0.5674 - precision_5: 0.5775 - recall_5: 0.2874 - auc_5: 0.6143\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.8099 - accuracy: 0.5520 - precision_5: 0.5446 - recall_5: 0.2710 - auc_5: 0.6064\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.8043 - accuracy: 0.5498 - precision_5: 0.5368 - recall_5: 0.2897 - auc_5: 0.5963\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7837 - accuracy: 0.5783 - precision_5: 0.5609 - recall_5: 0.4626 - auc_5: 0.6261\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7582 - accuracy: 0.6123 - precision_5: 0.6057 - recall_5: 0.4953 - auc_5: 0.6804\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7633 - accuracy: 0.5728 - precision_5: 0.5450 - recall_5: 0.5374 - auc_5: 0.6258\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.7687 - accuracy: 0.5816 - precision_5: 0.5567 - recall_5: 0.5280 - auc_5: 0.6106\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.7510 - accuracy: 0.6090 - precision_5: 0.5851 - recall_5: 0.5701 - auc_5: 0.6470\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7275 - accuracy: 0.6199 - precision_5: 0.6104 - recall_5: 0.5234 - auc_5: 0.6806\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.6875 - accuracy: 0.6824 - precision_5: 0.6700 - recall_5: 0.6355 - auc_5: 0.7455\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.6620 - accuracy: 0.7076 - precision_5: 0.6903 - recall_5: 0.6822 - auc_5: 0.7703\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.6005 - accuracy: 0.7349 - precision_5: 0.7598 - recall_5: 0.6355 - auc_5: 0.8303\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.5398 - accuracy: 0.7963 - precision_5: 0.7665 - recall_5: 0.8131 - auc_5: 0.8639\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.4799 - accuracy: 0.8346 - precision_5: 0.8259 - recall_5: 0.8201 - auc_5: 0.8963\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.4509 - accuracy: 0.8346 - precision_5: 0.8169 - recall_5: 0.8341 - auc_5: 0.9085\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.4274 - accuracy: 0.8543 - precision_5: 0.8145 - recall_5: 0.8925 - auc_5: 0.9186\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.3799 - accuracy: 0.8697 - precision_5: 0.8815 - recall_5: 0.8341 - auc_5: 0.9394\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.3216 - accuracy: 0.9135 - precision_5: 0.9106 - recall_5: 0.9042 - auc_5: 0.9588\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.3721 - accuracy: 0.8817 - precision_5: 0.8265 - recall_5: 0.9463 - auc_5: 0.9437\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.2874 - accuracy: 0.9157 - precision_5: 0.9091 - recall_5: 0.9112 - auc_5: 0.9665\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.3040 - accuracy: 0.9025 - precision_5: 0.9270 - recall_5: 0.8598 - auc_5: 0.9643\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2699 - accuracy: 0.9146 - precision_5: 0.9070 - recall_5: 0.9112 - auc_5: 0.9689\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.2598 - accuracy: 0.9113 - precision_5: 0.8916 - recall_5: 0.9229 - auc_5: 0.9726\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.2503 - accuracy: 0.9255 - precision_5: 0.9167 - recall_5: 0.9252 - auc_5: 0.9715\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.2157 - accuracy: 0.9419 - precision_5: 0.9562 - recall_5: 0.9182 - auc_5: 0.9809\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.2854 - accuracy: 0.9102 - precision_5: 0.8932 - recall_5: 0.9182 - auc_5: 0.9635\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.2713 - accuracy: 0.9135 - precision_5: 0.8785 - recall_5: 0.9463 - auc_5: 0.9674\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.2209 - accuracy: 0.9376 - precision_5: 0.9673 - recall_5: 0.8972 - auc_5: 0.9813\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.1888 - accuracy: 0.9452 - precision_5: 0.9127 - recall_5: 0.9766 - auc_5: 0.9880\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1711 - accuracy: 0.9606 - precision_5: 0.9712 - recall_5: 0.9439 - auc_5: 0.9862\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1887 - accuracy: 0.9452 - precision_5: 0.9163 - recall_5: 0.9720 - auc_5: 0.9841\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7124374620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.3539 - accuracy: 0.4885 - precision_6: 0.4558 - recall_6: 0.4696 - auc_6: 0.5142\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 1.2942 - accuracy: 0.5060 - precision_6: 0.4709 - recall_6: 0.4346 - auc_6: 0.5042\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.2464 - accuracy: 0.5137 - precision_6: 0.4692 - recall_6: 0.2850 - auc_6: 0.4920\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.1969 - accuracy: 0.4995 - precision_6: 0.4598 - recall_6: 0.3879 - auc_6: 0.4961\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 1.1522 - accuracy: 0.5225 - precision_6: 0.4878 - recall_6: 0.3738 - auc_6: 0.5233\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.1173 - accuracy: 0.5301 - precision_6: 0.4983 - recall_6: 0.3411 - auc_6: 0.5102\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.0912 - accuracy: 0.4885 - precision_6: 0.4156 - recall_6: 0.2243 - auc_6: 0.4668\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.0471 - accuracy: 0.5312 - precision_6: 0.5000 - recall_6: 0.2196 - auc_6: 0.5277\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 1.0229 - accuracy: 0.5159 - precision_6: 0.4698 - recall_6: 0.2547 - auc_6: 0.4976\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.9970 - accuracy: 0.5137 - precision_6: 0.4680 - recall_6: 0.2734 - auc_6: 0.4909\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.9660 - accuracy: 0.5268 - precision_6: 0.4912 - recall_6: 0.2617 - auc_6: 0.5257\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.9473 - accuracy: 0.5038 - precision_6: 0.4585 - recall_6: 0.3224 - auc_6: 0.5013\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.9312 - accuracy: 0.4995 - precision_6: 0.4617 - recall_6: 0.4089 - auc_6: 0.4846\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.9097 - accuracy: 0.5257 - precision_6: 0.4920 - recall_6: 0.3575 - auc_6: 0.4983\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.8892 - accuracy: 0.5214 - precision_6: 0.4871 - recall_6: 0.3972 - auc_6: 0.5189\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.8680 - accuracy: 0.5520 - precision_6: 0.5345 - recall_6: 0.3435 - auc_6: 0.5528\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.8569 - accuracy: 0.5498 - precision_6: 0.5368 - recall_6: 0.2897 - auc_6: 0.5307\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.8455 - accuracy: 0.5531 - precision_6: 0.5385 - recall_6: 0.3271 - auc_6: 0.5259\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.8364 - accuracy: 0.5082 - precision_6: 0.4621 - recall_6: 0.2991 - auc_6: 0.5003\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.8243 - accuracy: 0.5192 - precision_6: 0.4828 - recall_6: 0.3598 - auc_6: 0.5161\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.8165 - accuracy: 0.4995 - precision_6: 0.4621 - recall_6: 0.4136 - auc_6: 0.5006\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.8054 - accuracy: 0.5148 - precision_6: 0.4737 - recall_6: 0.3154 - auc_6: 0.5116\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7996 - accuracy: 0.5060 - precision_6: 0.4327 - recall_6: 0.1729 - auc_6: 0.4907\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7879 - accuracy: 0.5356 - precision_6: 0.5244 - recall_6: 0.1005 - auc_6: 0.5244\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.7799 - accuracy: 0.5334 - precision_6: 0.5152 - recall_6: 0.0794 - auc_6: 0.5417\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7756 - accuracy: 0.5225 - precision_6: 0.4789 - recall_6: 0.2126 - auc_6: 0.5115\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7715 - accuracy: 0.5115 - precision_6: 0.4750 - recall_6: 0.3995 - auc_6: 0.5059\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7683 - accuracy: 0.4951 - precision_6: 0.4660 - recall_6: 0.5280 - auc_6: 0.4883\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7613 - accuracy: 0.5071 - precision_6: 0.4673 - recall_6: 0.3668 - auc_6: 0.4973\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7546 - accuracy: 0.5214 - precision_6: 0.4759 - recall_6: 0.2079 - auc_6: 0.4990\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7490 - accuracy: 0.5323 - precision_6: 0.5023 - recall_6: 0.2570 - auc_6: 0.5259\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7500 - accuracy: 0.5104 - precision_6: 0.4708 - recall_6: 0.3575 - auc_6: 0.4817\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7429 - accuracy: 0.5126 - precision_6: 0.4732 - recall_6: 0.3505 - auc_6: 0.5200\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 0.7417 - accuracy: 0.5192 - precision_6: 0.4678 - recall_6: 0.1869 - auc_6: 0.4877\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.7337 - accuracy: 0.5367 - precision_6: 0.5258 - recall_6: 0.1192 - auc_6: 0.5343\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7326 - accuracy: 0.5389 - precision_6: 0.5412 - recall_6: 0.1075 - auc_6: 0.5227\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.7338 - accuracy: 0.5126 - precision_6: 0.4380 - recall_6: 0.1402 - auc_6: 0.4805\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.7304 - accuracy: 0.5126 - precision_6: 0.4555 - recall_6: 0.2033 - auc_6: 0.4984\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.7283 - accuracy: 0.5159 - precision_6: 0.4620 - recall_6: 0.1986 - auc_6: 0.4916\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.7270 - accuracy: 0.5192 - precision_6: 0.4772 - recall_6: 0.2687 - auc_6: 0.4865\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.7215 - accuracy: 0.5268 - precision_6: 0.4924 - recall_6: 0.3037 - auc_6: 0.5326\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.7217 - accuracy: 0.5203 - precision_6: 0.4684 - recall_6: 0.1729 - auc_6: 0.4889\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.7195 - accuracy: 0.5148 - precision_6: 0.4645 - recall_6: 0.2290 - auc_6: 0.5040\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.7189 - accuracy: 0.5356 - precision_6: 0.5137 - recall_6: 0.1752 - auc_6: 0.4772\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7161 - accuracy: 0.5170 - precision_6: 0.4504 - recall_6: 0.1379 - auc_6: 0.5074\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.7168 - accuracy: 0.5225 - precision_6: 0.4487 - recall_6: 0.0818 - auc_6: 0.4742\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7104 - accuracy: 0.5378 - precision_6: 0.6250 - recall_6: 0.0350 - auc_6: 0.5441\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.7139 - accuracy: 0.5301 - precision_6: 0.4615 - recall_6: 0.0140 - auc_6: 0.4940\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7122 - accuracy: 0.5170 - precision_6: 0.4217 - recall_6: 0.0818 - auc_6: 0.4875\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.7099 - accuracy: 0.5115 - precision_6: 0.4308 - recall_6: 0.1308 - auc_6: 0.5011\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7124f36d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 1.3784 - accuracy: 0.5027 - precision_7: 0.4747 - recall_7: 0.5701 - auc_7: 0.5168\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.3080 - accuracy: 0.5071 - precision_7: 0.4604 - recall_7: 0.2991 - auc_7: 0.5280\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 1.2519 - accuracy: 0.5137 - precision_7: 0.4770 - recall_7: 0.3879 - auc_7: 0.5412\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 1.1970 - accuracy: 0.5444 - precision_7: 0.5168 - recall_7: 0.4322 - auc_7: 0.5733\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.1665 - accuracy: 0.5444 - precision_7: 0.5294 - recall_7: 0.2523 - auc_7: 0.5563\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.1108 - accuracy: 0.5608 - precision_7: 0.5408 - recall_7: 0.4182 - auc_7: 0.5971\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 1.0744 - accuracy: 0.5739 - precision_7: 0.5397 - recall_7: 0.6192 - auc_7: 0.6097\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 1.0407 - accuracy: 0.5827 - precision_7: 0.5607 - recall_7: 0.5070 - auc_7: 0.6116\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 1.0103 - accuracy: 0.5597 - precision_7: 0.5280 - recall_7: 0.5724 - auc_7: 0.6073\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.9643 - accuracy: 0.5728 - precision_7: 0.5699 - recall_7: 0.3621 - auc_7: 0.6570\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.9215 - accuracy: 0.6079 - precision_7: 0.6207 - recall_7: 0.4206 - auc_7: 0.7014\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.8913 - accuracy: 0.6320 - precision_7: 0.6337 - recall_7: 0.5093 - auc_7: 0.7116\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.8341 - accuracy: 0.7097 - precision_7: 0.6674 - recall_7: 0.7593 - auc_7: 0.7769\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7662 - accuracy: 0.7820 - precision_7: 0.7528 - recall_7: 0.7967 - auc_7: 0.8364\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.7068 - accuracy: 0.8028 - precision_7: 0.7672 - recall_7: 0.8318 - auc_7: 0.8556\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.6045 - accuracy: 0.8521 - precision_7: 0.8124 - recall_7: 0.8902 - auc_7: 0.9126\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.5674 - accuracy: 0.8686 - precision_7: 0.8143 - recall_7: 0.9322 - auc_7: 0.9113\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.5667 - accuracy: 0.8434 - precision_7: 0.8188 - recall_7: 0.8551 - auc_7: 0.9042\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.6403 - accuracy: 0.8116 - precision_7: 0.7270 - recall_7: 0.9579 - auc_7: 0.8544\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.5591 - accuracy: 0.8248 - precision_7: 0.8508 - recall_7: 0.7593 - auc_7: 0.9038\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.4776 - accuracy: 0.8883 - precision_7: 0.8196 - recall_7: 0.9766 - auc_7: 0.9247\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3999 - accuracy: 0.9222 - precision_7: 0.8872 - recall_7: 0.9556 - auc_7: 0.9538\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.3765 - accuracy: 0.9168 - precision_7: 0.8793 - recall_7: 0.9533 - auc_7: 0.9560\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3748 - accuracy: 0.9146 - precision_7: 0.8616 - recall_7: 0.9743 - auc_7: 0.9534\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.4240 - accuracy: 0.8872 - precision_7: 0.8407 - recall_7: 0.9369 - auc_7: 0.9317\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3953 - accuracy: 0.8894 - precision_7: 0.8811 - recall_7: 0.8832 - auc_7: 0.9500: 0s - loss: 0.3952 - accuracy: 0.8887 - precision_7: 0.8433 - recall_7: 0.9378 - auc_7: 0.\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.3734 - accuracy: 0.8916 - precision_7: 0.8364 - recall_7: 0.9556 - auc_7: 0.9456\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.3381 - accuracy: 0.9047 - precision_7: 0.8920 - recall_7: 0.9065 - auc_7: 0.9635\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.3464 - accuracy: 0.8981 - precision_7: 0.8665 - recall_7: 0.9252 - auc_7: 0.9571\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.3091 - accuracy: 0.9222 - precision_7: 0.9048 - recall_7: 0.9322 - auc_7: 0.9644\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.2312 - accuracy: 0.9595 - precision_7: 0.9474 - recall_7: 0.9673 - auc_7: 0.9835\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.2471 - accuracy: 0.9398 - precision_7: 0.9172 - recall_7: 0.9579 - auc_7: 0.9824\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2465 - accuracy: 0.9441 - precision_7: 0.9089 - recall_7: 0.9790 - auc_7: 0.9756\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.2481 - accuracy: 0.9387 - precision_7: 0.9247 - recall_7: 0.9463 - auc_7: 0.9764\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2417 - accuracy: 0.9409 - precision_7: 0.9539 - recall_7: 0.9182 - auc_7: 0.9808\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.2749 - accuracy: 0.9244 - precision_7: 0.8732 - recall_7: 0.9813 - auc_7: 0.9726\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.2394 - accuracy: 0.9321 - precision_7: 0.9085 - recall_7: 0.9509 - auc_7: 0.9794\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.3306 - accuracy: 0.8927 - precision_7: 0.8327 - recall_7: 0.9650 - auc_7: 0.9567\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.3159 - accuracy: 0.8981 - precision_7: 0.8512 - recall_7: 0.9486 - auc_7: 0.9522\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.3770 - accuracy: 0.8620 - precision_7: 0.8595 - recall_7: 0.8435 - auc_7: 0.9348\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.2433 - accuracy: 0.9332 - precision_7: 0.9051 - recall_7: 0.9579 - auc_7: 0.9739\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.2090 - accuracy: 0.9518 - precision_7: 0.9364 - recall_7: 0.9626 - auc_7: 0.9817\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.1931 - accuracy: 0.9496 - precision_7: 0.9226 - recall_7: 0.9743 - auc_7: 0.9830\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.2255 - accuracy: 0.9409 - precision_7: 0.9495 - recall_7: 0.9229 - auc_7: 0.9803\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.2003 - accuracy: 0.9452 - precision_7: 0.9437 - recall_7: 0.9393 - auc_7: 0.9845\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.1929 - accuracy: 0.9485 - precision_7: 0.9320 - recall_7: 0.9603 - auc_7: 0.9824\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.1894 - accuracy: 0.9452 - precision_7: 0.9200 - recall_7: 0.9673 - auc_7: 0.9838\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1465 - accuracy: 0.9671 - precision_7: 0.9502 - recall_7: 0.9813 - auc_7: 0.9913\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1355 - accuracy: 0.9726 - precision_7: 0.9569 - recall_7: 0.9860 - auc_7: 0.9929\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1199 - accuracy: 0.9770 - precision_7: 0.9722 - recall_7: 0.9790 - auc_7: 0.9951\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f71247586a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 1.4121 - accuracy: 0.4556 - precision_8: 0.4017 - recall_8: 0.3294 - auc_8: 0.4486\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.3006 - accuracy: 0.5400 - precision_8: 0.5105 - recall_8: 0.4556 - auc_8: 0.5416\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 1.2597 - accuracy: 0.5170 - precision_8: 0.4801 - recall_8: 0.3668 - auc_8: 0.5114\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 1.2240 - accuracy: 0.4896 - precision_8: 0.4451 - recall_8: 0.3598 - auc_8: 0.4770\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.1678 - accuracy: 0.5356 - precision_8: 0.5053 - recall_8: 0.4439 - auc_8: 0.5379\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 1.1363 - accuracy: 0.5049 - precision_8: 0.4565 - recall_8: 0.2944 - auc_8: 0.5041\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.1004 - accuracy: 0.5082 - precision_8: 0.4591 - recall_8: 0.2757 - auc_8: 0.5090\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.0603 - accuracy: 0.5268 - precision_8: 0.4936 - recall_8: 0.3621 - auc_8: 0.5335\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.0338 - accuracy: 0.5279 - precision_8: 0.4960 - recall_8: 0.4346 - auc_8: 0.5259\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 1.0082 - accuracy: 0.5027 - precision_8: 0.4711 - recall_8: 0.4953 - auc_8: 0.5134\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.9790 - accuracy: 0.5235 - precision_8: 0.4881 - recall_8: 0.3364 - auc_8: 0.5264\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.9603 - accuracy: 0.5137 - precision_8: 0.4692 - recall_8: 0.2850 - auc_8: 0.5023\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.9350 - accuracy: 0.5137 - precision_8: 0.4722 - recall_8: 0.3178 - auc_8: 0.5175\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.9121 - accuracy: 0.5279 - precision_8: 0.4942 - recall_8: 0.2967 - auc_8: 0.5377\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.8916 - accuracy: 0.5126 - precision_8: 0.4597 - recall_8: 0.2266 - auc_8: 0.5366\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.8785 - accuracy: 0.5378 - precision_8: 0.5195 - recall_8: 0.1869 - auc_8: 0.5404\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.8527 - accuracy: 0.5652 - precision_8: 0.5671 - recall_8: 0.3061 - auc_8: 0.5796\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.8454 - accuracy: 0.5181 - precision_8: 0.4816 - recall_8: 0.3668 - auc_8: 0.5541\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.8309 - accuracy: 0.5345 - precision_8: 0.5035 - recall_8: 0.5000 - auc_8: 0.5680\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.8092 - accuracy: 0.5860 - precision_8: 0.5568 - recall_8: 0.5724 - auc_8: 0.6148\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7862 - accuracy: 0.6090 - precision_8: 0.5820 - recall_8: 0.5888 - auc_8: 0.6537\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7777 - accuracy: 0.5652 - precision_8: 0.5649 - recall_8: 0.3154 - auc_8: 0.6276\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7653 - accuracy: 0.5465 - precision_8: 0.5271 - recall_8: 0.3178 - auc_8: 0.6205\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.7542 - accuracy: 0.5739 - precision_8: 0.5552 - recall_8: 0.4579 - auc_8: 0.6386\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7540 - accuracy: 0.5696 - precision_8: 0.5362 - recall_8: 0.6051 - auc_8: 0.5981\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.7212 - accuracy: 0.5783 - precision_8: 0.5483 - recall_8: 0.5701 - auc_8: 0.6469\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7207 - accuracy: 0.6156 - precision_8: 0.5807 - recall_8: 0.6472 - auc_8: 0.6653\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.6643 - accuracy: 0.6977 - precision_8: 0.6514 - recall_8: 0.7640 - auc_8: 0.7586\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.6816 - accuracy: 0.6736 - precision_8: 0.6012 - recall_8: 0.9019 - auc_8: 0.7289\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.6876 - accuracy: 0.6506 - precision_8: 0.6062 - recall_8: 0.7266 - auc_8: 0.6964\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.6872 - accuracy: 0.6287 - precision_8: 0.5854 - recall_8: 0.7126 - auc_8: 0.6760: 0s - loss: 0.6977 - accuracy: 0.6055 - precision_8: 0.5461 - recall_8: 0.6754 - auc_8: \n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.6524 - accuracy: 0.6451 - precision_8: 0.6238 - recall_8: 0.6121 - auc_8: 0.7342\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.6162 - accuracy: 0.6944 - precision_8: 0.6976 - recall_8: 0.6145 - auc_8: 0.7823\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.5713 - accuracy: 0.7547 - precision_8: 0.7032 - recall_8: 0.8248 - auc_8: 0.8258\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.5556 - accuracy: 0.7689 - precision_8: 0.7256 - recall_8: 0.8154 - auc_8: 0.8305\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.5642 - accuracy: 0.7623 - precision_8: 0.7193 - recall_8: 0.8084 - auc_8: 0.8252\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 0.4819 - accuracy: 0.8237 - precision_8: 0.8156 - recall_8: 0.8061 - auc_8: 0.8879\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.4488 - accuracy: 0.8357 - precision_8: 0.7872 - recall_8: 0.8902 - auc_8: 0.8994\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.4168 - accuracy: 0.8445 - precision_8: 0.8082 - recall_8: 0.8762 - auc_8: 0.9118\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.3687 - accuracy: 0.8620 - precision_8: 0.8240 - recall_8: 0.8972 - auc_8: 0.9347\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.3910 - accuracy: 0.8609 - precision_8: 0.8444 - recall_8: 0.8621 - auc_8: 0.9253\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 0.3414 - accuracy: 0.8949 - precision_8: 0.8578 - recall_8: 0.9299 - auc_8: 0.9430\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 0.3267 - accuracy: 0.8850 - precision_8: 0.8386 - recall_8: 0.9346 - auc_8: 0.9509\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.3460 - accuracy: 0.8850 - precision_8: 0.8713 - recall_8: 0.8855 - auc_8: 0.9410\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.3051 - accuracy: 0.8959 - precision_8: 0.8675 - recall_8: 0.9182 - auc_8: 0.9544\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.2501 - accuracy: 0.9266 - precision_8: 0.8985 - recall_8: 0.9509 - auc_8: 0.9702\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.2480 - accuracy: 0.9244 - precision_8: 0.8980 - recall_8: 0.9463 - auc_8: 0.9719\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.2548 - accuracy: 0.9277 - precision_8: 0.9190 - recall_8: 0.9276 - auc_8: 0.9676\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2517 - accuracy: 0.9255 - precision_8: 0.9206 - recall_8: 0.9206 - auc_8: 0.9701\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3144 - accuracy: 0.8817 - precision_8: 0.8113 - recall_8: 0.9743 - auc_8: 0.9587\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7124c09ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 1.3655 - accuracy: 0.5235 - precision_9: 0.4923 - recall_9: 0.5257 - auc_9: 0.5308\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.3207 - accuracy: 0.5016 - precision_9: 0.4703 - recall_9: 0.5000 - auc_9: 0.4979\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 1.2506 - accuracy: 0.5115 - precision_9: 0.4746 - recall_9: 0.3925 - auc_9: 0.5365\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.1907 - accuracy: 0.5487 - precision_9: 0.5296 - recall_9: 0.3341 - auc_9: 0.5869\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 1.1465 - accuracy: 0.5553 - precision_9: 0.5251 - recall_9: 0.5374 - auc_9: 0.5933\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 1.1139 - accuracy: 0.5641 - precision_9: 0.5295 - recall_9: 0.6285 - auc_9: 0.5976\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 1.0736 - accuracy: 0.5674 - precision_9: 0.5457 - recall_9: 0.4603 - auc_9: 0.6016\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 1.0148 - accuracy: 0.6068 - precision_9: 0.5915 - recall_9: 0.5210 - auc_9: 0.6685\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.9770 - accuracy: 0.6199 - precision_9: 0.6041 - recall_9: 0.5491 - auc_9: 0.6774\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.8926 - accuracy: 0.6966 - precision_9: 0.7163 - recall_9: 0.5841 - auc_9: 0.7902\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.8689 - accuracy: 0.7010 - precision_9: 0.6362 - recall_9: 0.8458 - auc_9: 0.7729\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.8651 - accuracy: 0.6670 - precision_9: 0.6260 - recall_9: 0.7196 - auc_9: 0.7332\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.8054 - accuracy: 0.7119 - precision_9: 0.6701 - recall_9: 0.7593 - auc_9: 0.7860\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.7498 - accuracy: 0.7459 - precision_9: 0.7149 - recall_9: 0.7617 - auc_9: 0.8187\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.6630 - accuracy: 0.7952 - precision_9: 0.7720 - recall_9: 0.7991 - auc_9: 0.8762\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.5644 - accuracy: 0.8598 - precision_9: 0.8589 - recall_9: 0.8388 - auc_9: 0.9295\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.5015 - accuracy: 0.8828 - precision_9: 0.8437 - recall_9: 0.9206 - auc_9: 0.9397\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.4592 - accuracy: 0.8894 - precision_9: 0.8691 - recall_9: 0.8995 - auc_9: 0.9525\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.4228 - accuracy: 0.9003 - precision_9: 0.8946 - recall_9: 0.8925 - auc_9: 0.9596\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3590 - accuracy: 0.9211 - precision_9: 0.8938 - recall_9: 0.9439 - auc_9: 0.9731\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3258 - accuracy: 0.9376 - precision_9: 0.9264 - recall_9: 0.9416 - auc_9: 0.9797\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.3393 - accuracy: 0.9266 - precision_9: 0.9247 - recall_9: 0.9182 - auc_9: 0.9751\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.3001 - accuracy: 0.9409 - precision_9: 0.9174 - recall_9: 0.9603 - auc_9: 0.9819\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 0.2514 - accuracy: 0.9518 - precision_9: 0.9384 - recall_9: 0.9603 - auc_9: 0.9899\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.2353 - accuracy: 0.9551 - precision_9: 0.9618 - recall_9: 0.9416 - auc_9: 0.9914\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.2254 - accuracy: 0.9584 - precision_9: 0.9333 - recall_9: 0.9813 - auc_9: 0.9893\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.2586 - accuracy: 0.9474 - precision_9: 0.9398 - recall_9: 0.9486 - auc_9: 0.9828\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.1956 - accuracy: 0.9682 - precision_9: 0.9629 - recall_9: 0.9696 - auc_9: 0.9933\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.3043 - accuracy: 0.9200 - precision_9: 0.9610 - recall_9: 0.8645 - auc_9: 0.9763\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 0.3140 - accuracy: 0.9080 - precision_9: 0.8857 - recall_9: 0.9229 - auc_9: 0.9640\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.2632 - accuracy: 0.9321 - precision_9: 0.8996 - recall_9: 0.9626 - auc_9: 0.9771\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.2198 - accuracy: 0.9441 - precision_9: 0.9333 - recall_9: 0.9486 - auc_9: 0.9863\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.2017 - accuracy: 0.9573 - precision_9: 0.9313 - recall_9: 0.9813 - auc_9: 0.9870\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.2035 - accuracy: 0.9507 - precision_9: 0.9705 - recall_9: 0.9229 - auc_9: 0.9895\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.1810 - accuracy: 0.9606 - precision_9: 0.9434 - recall_9: 0.9743 - auc_9: 0.9912\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.1397 - accuracy: 0.9781 - precision_9: 0.9811 - recall_9: 0.9720 - auc_9: 0.9966\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.1642 - accuracy: 0.9639 - precision_9: 0.9341 - recall_9: 0.9930 - auc_9: 0.9938\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.1449 - accuracy: 0.9759 - precision_9: 0.9903 - recall_9: 0.9579 - auc_9: 0.9940\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 0.1222 - accuracy: 0.9737 - precision_9: 0.9612 - recall_9: 0.9836 - auc_9: 0.9964\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.1108 - accuracy: 0.9792 - precision_9: 0.9789 - recall_9: 0.9766 - auc_9: 0.9977\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.1359 - accuracy: 0.9682 - precision_9: 0.9483 - recall_9: 0.9860 - auc_9: 0.9940\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.1016 - accuracy: 0.9869 - precision_9: 0.9883 - recall_9: 0.9836 - auc_9: 0.9970\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.1135 - accuracy: 0.9770 - precision_9: 0.9678 - recall_9: 0.9836 - auc_9: 0.9956\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 0.1042 - accuracy: 0.9759 - precision_9: 0.9721 - recall_9: 0.9766 - auc_9: 0.9976\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.1044 - accuracy: 0.9792 - precision_9: 0.9723 - recall_9: 0.9836 - auc_9: 0.9965\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 84ms/step - loss: 0.0826 - accuracy: 0.9858 - precision_9: 0.9770 - recall_9: 0.9930 - auc_9: 0.9982\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 0.0797 - accuracy: 0.9847 - precision_9: 0.9770 - recall_9: 0.9907 - auc_9: 0.9988\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.0921 - accuracy: 0.9814 - precision_9: 0.9835 - recall_9: 0.9766 - auc_9: 0.9958\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.0869 - accuracy: 0.9847 - precision_9: 0.9726 - recall_9: 0.9953 - auc_9: 0.9973\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0771 - accuracy: 0.9858 - precision_9: 0.9952 - recall_9: 0.9743 - auc_9: 0.9979\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f71245f1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 73.450% (+/-10.354) \n",
      " Precision: 59.650% (+/-32.926) \n",
      " Recall: 46.818% (+/-27.144) \n",
      " AUC: 79.458% (+/-16.535) \n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_2(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 1, 50, 128\n",
    "    n_features = trainX.shape[2]\n",
    "    n_steps, n_length = 10, 625\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3),input_shape=(None,n_length,n_features)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(50,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01) ))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation= 'sigmoid' ))\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=['accuracy',Precision(),Recall(),AUC()])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy,precision,recall,auc = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy,precision,recall,auc\n",
    "\n",
    "def run_experiment_2(X,y,repeats=5):\n",
    "    # load data\n",
    "    trainX, testX,trainy, testy = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    # repeat experiment\n",
    "    accuracies = list()\n",
    "    precisions = list()\n",
    "    recalls = list()\n",
    "    aucs = list()\n",
    "    for r in range(repeats):\n",
    "        accuracy,precision,recall,auc = evaluate_model_2(trainX, trainy, testX, testy)\n",
    "        accuracy = accuracy * 100.0\n",
    "        precision = precision * 100.0\n",
    "        recall = recall * 100.0\n",
    "        auc = auc * 100.0\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        aucs.append(auc)\n",
    "    # summarize results\n",
    "    summarize_results(accuracies,precisions,recalls,aucs)\n",
    "    \n",
    "run_experiment_2(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 1.5910 - accuracy: 0.4896 - precision_10: 0.4599 - recall_10: 0.5093 - auc_10: 0.4792\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.4102 - accuracy: 0.5192 - precision_10: 0.4887 - recall_10: 0.5561 - auc_10: 0.5433\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.4189 - accuracy: 0.5049 - precision_10: 0.4608 - recall_10: 0.3294 - auc_10: 0.5048\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.3581 - accuracy: 0.5203 - precision_10: 0.4870 - recall_10: 0.4369 - auc_10: 0.5161\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.2987 - accuracy: 0.5257 - precision_10: 0.4943 - recall_10: 0.5070 - auc_10: 0.5407\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 1.2812 - accuracy: 0.5279 - precision_10: 0.4964 - recall_10: 0.4790 - auc_10: 0.5363\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 1.2621 - accuracy: 0.5192 - precision_10: 0.4885 - recall_10: 0.5444 - auc_10: 0.5143\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 86ms/step - loss: 1.2219 - accuracy: 0.5257 - precision_10: 0.4941 - recall_10: 0.4860 - auc_10: 0.5293\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.1809 - accuracy: 0.5465 - precision_10: 0.5167 - recall_10: 0.5047 - auc_10: 0.5628\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.1681 - accuracy: 0.5498 - precision_10: 0.5193 - recall_10: 0.5350 - auc_10: 0.5609\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 1.1316 - accuracy: 0.5696 - precision_10: 0.5513 - recall_10: 0.4393 - auc_10: 0.5943\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.1061 - accuracy: 0.5520 - precision_10: 0.5168 - recall_10: 0.6846 - auc_10: 0.6118\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 1.0940 - accuracy: 0.5685 - precision_10: 0.5503 - recall_10: 0.4346 - auc_10: 0.5966\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.0761 - accuracy: 0.5706 - precision_10: 0.5419 - recall_10: 0.5444 - auc_10: 0.6024\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 1.0719 - accuracy: 0.5465 - precision_10: 0.5235 - recall_10: 0.3645 - auc_10: 0.5694\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.0723 - accuracy: 0.4962 - precision_10: 0.4621 - recall_10: 0.4556 - auc_10: 0.5232\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.0169 - accuracy: 0.5674 - precision_10: 0.5442 - recall_10: 0.4743 - auc_10: 0.6006\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.9957 - accuracy: 0.5696 - precision_10: 0.5434 - recall_10: 0.5117 - auc_10: 0.6117\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.9902 - accuracy: 0.5860 - precision_10: 0.5525 - recall_10: 0.6145 - auc_10: 0.6325\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.9496 - accuracy: 0.6210 - precision_10: 0.6046 - recall_10: 0.5537 - auc_10: 0.6665\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 0.8827 - accuracy: 0.6846 - precision_10: 0.7023 - recall_10: 0.5678 - auc_10: 0.7493\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.8329 - accuracy: 0.7032 - precision_10: 0.6987 - recall_10: 0.6449 - auc_10: 0.7872\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.8026 - accuracy: 0.7207 - precision_10: 0.6667 - recall_10: 0.8084 - auc_10: 0.8173\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7521 - accuracy: 0.7711 - precision_10: 0.7786 - recall_10: 0.7150 - auc_10: 0.8429\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.6794 - accuracy: 0.8105 - precision_10: 0.8474 - recall_10: 0.7266 - auc_10: 0.8854\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.6214 - accuracy: 0.8368 - precision_10: 0.8149 - recall_10: 0.8435 - auc_10: 0.9142\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.6079 - accuracy: 0.8248 - precision_10: 0.8564 - recall_10: 0.7523 - auc_10: 0.9192\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.6305 - accuracy: 0.8105 - precision_10: 0.8400 - recall_10: 0.7360 - auc_10: 0.9003\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.6254 - accuracy: 0.8171 - precision_10: 0.8145 - recall_10: 0.7897 - auc_10: 0.9001\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.5828 - accuracy: 0.8445 - precision_10: 0.7930 - recall_10: 0.9042 - auc_10: 0.9290\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.5563 - accuracy: 0.8521 - precision_10: 0.9058 - recall_10: 0.7640 - auc_10: 0.9305\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.4811 - accuracy: 0.8861 - precision_10: 0.8600 - recall_10: 0.9042 - auc_10: 0.9552\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.5021 - accuracy: 0.8916 - precision_10: 0.9229 - recall_10: 0.8388 - auc_10: 0.9410\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.5014 - accuracy: 0.8697 - precision_10: 0.8160 - recall_10: 0.9322 - auc_10: 0.9525\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.4535 - accuracy: 0.8959 - precision_10: 0.8955 - recall_10: 0.8808 - auc_10: 0.9563\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.4290 - accuracy: 0.9014 - precision_10: 0.9495 - recall_10: 0.8341 - auc_10: 0.9642\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.3731 - accuracy: 0.9288 - precision_10: 0.9373 - recall_10: 0.9089 - auc_10: 0.9734\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.3548 - accuracy: 0.9343 - precision_10: 0.9240 - recall_10: 0.9369 - auc_10: 0.9804\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.3395 - accuracy: 0.9430 - precision_10: 0.9476 - recall_10: 0.9299 - auc_10: 0.9798\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.3119 - accuracy: 0.9518 - precision_10: 0.9683 - recall_10: 0.9276 - auc_10: 0.9843\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.2920 - accuracy: 0.9518 - precision_10: 0.9571 - recall_10: 0.9393 - auc_10: 0.9870\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.2911 - accuracy: 0.9485 - precision_10: 0.9441 - recall_10: 0.9463 - auc_10: 0.9861\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.2940 - accuracy: 0.9540 - precision_10: 0.9595 - recall_10: 0.9416 - auc_10: 0.9842\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.3015 - accuracy: 0.9376 - precision_10: 0.9603 - recall_10: 0.9042 - auc_10: 0.9824\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.2747 - accuracy: 0.9595 - precision_10: 0.9622 - recall_10: 0.9509 - auc_10: 0.9863\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.2482 - accuracy: 0.9606 - precision_10: 0.9734 - recall_10: 0.9416 - auc_10: 0.9922\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.2413 - accuracy: 0.9573 - precision_10: 0.9598 - recall_10: 0.9486 - auc_10: 0.9902\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2452 - accuracy: 0.9617 - precision_10: 0.9758 - recall_10: 0.9416 - auc_10: 0.9885\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.2386 - accuracy: 0.9628 - precision_10: 0.9581 - recall_10: 0.9626 - auc_10: 0.9897\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2289 - accuracy: 0.9595 - precision_10: 0.9536 - recall_10: 0.9603 - auc_10: 0.9909\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7124f1c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 1.6798 - accuracy: 0.4973 - precision_11: 0.4653 - recall_11: 0.4860 - auc_11: 0.4932\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.5024 - accuracy: 0.5225 - precision_11: 0.4885 - recall_11: 0.3972 - auc_11: 0.5081\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 1.4543 - accuracy: 0.4973 - precision_11: 0.4727 - recall_11: 0.6262 - auc_11: 0.4953\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.4156 - accuracy: 0.5104 - precision_11: 0.4615 - recall_11: 0.2664 - auc_11: 0.5102\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 1.3347 - accuracy: 0.5389 - precision_11: 0.5086 - recall_11: 0.4813 - auc_11: 0.5518\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 1.2942 - accuracy: 0.5553 - precision_11: 0.5274 - recall_11: 0.4953 - auc_11: 0.5783\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 1.2569 - accuracy: 0.5685 - precision_11: 0.5427 - recall_11: 0.5047 - auc_11: 0.5898\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 1.2381 - accuracy: 0.5794 - precision_11: 0.5516 - recall_11: 0.5491 - auc_11: 0.6008\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 1.1827 - accuracy: 0.6112 - precision_11: 0.5847 - recall_11: 0.5888 - auc_11: 0.6464\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.1308 - accuracy: 0.6342 - precision_11: 0.6119 - recall_11: 0.6005 - auc_11: 0.6834\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 1.1013 - accuracy: 0.6484 - precision_11: 0.6777 - recall_11: 0.4766 - auc_11: 0.7279\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.0497 - accuracy: 0.6846 - precision_11: 0.6357 - recall_11: 0.7664 - auc_11: 0.7595\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.9824 - accuracy: 0.6977 - precision_11: 0.7065 - recall_11: 0.6075 - auc_11: 0.7961\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.8999 - accuracy: 0.7590 - precision_11: 0.7562 - recall_11: 0.7173 - auc_11: 0.8515\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9010 - accuracy: 0.7678 - precision_11: 0.7422 - recall_11: 0.7734 - auc_11: 0.8506\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7799 - accuracy: 0.8258 - precision_11: 0.8050 - recall_11: 0.8294 - auc_11: 0.9087\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.7716 - accuracy: 0.8478 - precision_11: 0.8233 - recall_11: 0.8598 - auc_11: 0.9114\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.6944 - accuracy: 0.8587 - precision_11: 0.8551 - recall_11: 0.8411 - auc_11: 0.9375\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.6870 - accuracy: 0.8740 - precision_11: 0.8648 - recall_11: 0.8668 - auc_11: 0.9356\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.6420 - accuracy: 0.8784 - precision_11: 0.8438 - recall_11: 0.9089 - auc_11: 0.9492\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.6372 - accuracy: 0.8872 - precision_11: 0.8495 - recall_11: 0.9229 - auc_11: 0.9461\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.5659 - accuracy: 0.9124 - precision_11: 0.9104 - recall_11: 0.9019 - auc_11: 0.9636\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.5696 - accuracy: 0.9124 - precision_11: 0.8973 - recall_11: 0.9182 - auc_11: 0.9587\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.5668 - accuracy: 0.8970 - precision_11: 0.8761 - recall_11: 0.9089 - auc_11: 0.9566\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.5318 - accuracy: 0.9189 - precision_11: 0.9097 - recall_11: 0.9182 - auc_11: 0.9640\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.4733 - accuracy: 0.9365 - precision_11: 0.9322 - recall_11: 0.9322 - auc_11: 0.9779 0s - loss: 0.4825 - accuracy: 0.9453 - precision_11: 0.9454 - recall_11: 0.9375 - auc_11\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.5205 - accuracy: 0.9080 - precision_11: 0.9410 - recall_11: 0.8575 - auc_11: 0.9663\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.4675 - accuracy: 0.9189 - precision_11: 0.9023 - recall_11: 0.9276 - auc_11: 0.9752\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.4341 - accuracy: 0.9376 - precision_11: 0.9113 - recall_11: 0.9603 - auc_11: 0.9813\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.5191 - accuracy: 0.9058 - precision_11: 0.8562 - recall_11: 0.9603 - auc_11: 0.9606\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.4870 - accuracy: 0.8992 - precision_11: 0.9264 - recall_11: 0.8528 - auc_11: 0.9656\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.4561 - accuracy: 0.9069 - precision_11: 0.9054 - recall_11: 0.8949 - auc_11: 0.9682\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4454 - accuracy: 0.9080 - precision_11: 0.8691 - recall_11: 0.9463 - auc_11: 0.9700\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.3633 - accuracy: 0.9474 - precision_11: 0.9460 - recall_11: 0.9416 - auc_11: 0.9877\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.3403 - accuracy: 0.9562 - precision_11: 0.9470 - recall_11: 0.9603 - auc_11: 0.9889\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.3215 - accuracy: 0.9628 - precision_11: 0.9560 - recall_11: 0.9650 - auc_11: 0.9894 0s - loss: 0.3168 - accuracy: 0.9531 - precision_11: 0.9268 - recall_11: 0.9744 - au\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.3100 - accuracy: 0.9573 - precision_11: 0.9471 - recall_11: 0.9626 - auc_11: 0.9913\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 0.3393 - accuracy: 0.9430 - precision_11: 0.9821 - recall_11: 0.8949 - auc_11: 0.9884\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.3153 - accuracy: 0.9584 - precision_11: 0.9221 - recall_11: 0.9953 - auc_11: 0.9872\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.2813 - accuracy: 0.9693 - precision_11: 0.9785 - recall_11: 0.9556 - auc_11: 0.9918\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.2436 - accuracy: 0.9737 - precision_11: 0.9633 - recall_11: 0.9813 - auc_11: 0.9970\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.2395 - accuracy: 0.9726 - precision_11: 0.9569 - recall_11: 0.9860 - auc_11: 0.9970\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 77ms/step - loss: 0.2491 - accuracy: 0.9693 - precision_11: 0.9785 - recall_11: 0.9556 - auc_11: 0.9946\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.2195 - accuracy: 0.9737 - precision_11: 0.9654 - recall_11: 0.9790 - auc_11: 0.9974\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.2217 - accuracy: 0.9748 - precision_11: 0.9655 - recall_11: 0.9813 - auc_11: 0.9965\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 0.2625 - accuracy: 0.9595 - precision_11: 0.9851 - recall_11: 0.9276 - auc_11: 0.9912\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.2894 - accuracy: 0.9398 - precision_11: 0.8894 - recall_11: 0.9953 - auc_11: 0.9909\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2421 - accuracy: 0.9584 - precision_11: 0.9779 - recall_11: 0.9322 - auc_11: 0.9951\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2916 - accuracy: 0.9452 - precision_11: 0.8971 - recall_11: 0.9977 - auc_11: 0.9864\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2092 - accuracy: 0.9759 - precision_11: 0.9927 - recall_11: 0.9556 - auc_11: 0.9970\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f712616fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.5232 - accuracy: 0.5049 - precision_12: 0.4740 - recall_12: 0.5117 - auc_12: 0.5027\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.4361 - accuracy: 0.5115 - precision_12: 0.4774 - recall_12: 0.4439 - auc_12: 0.4902\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.3632 - accuracy: 0.5334 - precision_12: 0.5026 - recall_12: 0.4439 - auc_12: 0.5258 0s - loss: 1.3951 - accuracy: 0.5016 - precision_12: 0.4737 - recall_12: 0.4131 - auc_12: \n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.3240 - accuracy: 0.5422 - precision_12: 0.5175 - recall_12: 0.3458 - auc_12: 0.5426\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.3179 - accuracy: 0.5148 - precision_12: 0.4834 - recall_12: 0.5093 - auc_12: 0.5071\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.2951 - accuracy: 0.5016 - precision_12: 0.4646 - recall_12: 0.4136 - auc_12: 0.4875\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.2375 - accuracy: 0.5159 - precision_12: 0.4802 - recall_12: 0.3972 - auc_12: 0.5195\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.2410 - accuracy: 0.4885 - precision_12: 0.4581 - recall_12: 0.4977 - auc_12: 0.4914\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.2238 - accuracy: 0.5137 - precision_12: 0.4708 - recall_12: 0.3014 - auc_12: 0.4871\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.1783 - accuracy: 0.5192 - precision_12: 0.4815 - recall_12: 0.3341 - auc_12: 0.5125\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.1472 - accuracy: 0.5170 - precision_12: 0.4863 - recall_12: 0.5397 - auc_12: 0.5298\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.1394 - accuracy: 0.4951 - precision_12: 0.4647 - recall_12: 0.5070 - auc_12: 0.5026\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 1.1231 - accuracy: 0.5060 - precision_12: 0.4672 - recall_12: 0.3832 - auc_12: 0.5035\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.1014 - accuracy: 0.5104 - precision_12: 0.4741 - recall_12: 0.4065 - auc_12: 0.4966\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 1.0759 - accuracy: 0.5170 - precision_12: 0.4831 - recall_12: 0.4346 - auc_12: 0.5263\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.0299 - accuracy: 0.5455 - precision_12: 0.5182 - recall_12: 0.4322 - auc_12: 0.5821\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.0390 - accuracy: 0.5487 - precision_12: 0.5229 - recall_12: 0.4276 - auc_12: 0.5567\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.9999 - accuracy: 0.5805 - precision_12: 0.5553 - recall_12: 0.5280 - auc_12: 0.6108\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.9850 - accuracy: 0.5871 - precision_12: 0.5617 - recall_12: 0.5421 - auc_12: 0.6122\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.9594 - accuracy: 0.6145 - precision_12: 0.6180 - recall_12: 0.4650 - auc_12: 0.6386\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.9219 - accuracy: 0.6561 - precision_12: 0.6557 - recall_12: 0.5607 - auc_12: 0.6872\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 0.9813 - accuracy: 0.5827 - precision_12: 0.5448 - recall_12: 0.6682 - auc_12: 0.6185\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.9158 - accuracy: 0.6331 - precision_12: 0.6703 - recall_12: 0.4276 - auc_12: 0.6853\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 0.8814 - accuracy: 0.6594 - precision_12: 0.6552 - recall_12: 0.5771 - auc_12: 0.6982\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.8996 - accuracy: 0.6166 - precision_12: 0.6354 - recall_12: 0.4276 - auc_12: 0.6582\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.8947 - accuracy: 0.6145 - precision_12: 0.5960 - recall_12: 0.5514 - auc_12: 0.6505\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.8279 - accuracy: 0.6747 - precision_12: 0.6379 - recall_12: 0.7079 - auc_12: 0.7425\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.7307 - accuracy: 0.7788 - precision_12: 0.8553 - recall_12: 0.6355 - auc_12: 0.8340\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.7013 - accuracy: 0.7886 - precision_12: 0.7664 - recall_12: 0.7897 - auc_12: 0.8524\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5820 - accuracy: 0.8532 - precision_12: 0.8889 - recall_12: 0.7850 - auc_12: 0.9183\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.5379 - accuracy: 0.8697 - precision_12: 0.8472 - recall_12: 0.8808 - auc_12: 0.9385\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.5370 - accuracy: 0.8751 - precision_12: 0.9005 - recall_12: 0.8248 - auc_12: 0.9292\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.5633 - accuracy: 0.8390 - precision_12: 0.8668 - recall_12: 0.7757 - auc_12: 0.9198\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.5843 - accuracy: 0.8357 - precision_12: 0.7780 - recall_12: 0.9089 - auc_12: 0.9253\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.5301 - accuracy: 0.8412 - precision_12: 0.8920 - recall_12: 0.7523 - auc_12: 0.9327\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.4711 - accuracy: 0.8773 - precision_12: 0.8333 - recall_12: 0.9229 - auc_12: 0.9510\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.4040 - accuracy: 0.9157 - precision_12: 0.9582 - recall_12: 0.8575 - auc_12: 0.9696\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.4289 - accuracy: 0.9025 - precision_12: 0.8645 - recall_12: 0.9393 - auc_12: 0.9602\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.5279 - accuracy: 0.8697 - precision_12: 0.9640 - recall_12: 0.7500 - auc_12: 0.9290\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.4488 - accuracy: 0.8992 - precision_12: 0.8605 - recall_12: 0.9369 - auc_12: 0.9545\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.3995 - accuracy: 0.9102 - precision_12: 0.9505 - recall_12: 0.8528 - auc_12: 0.9571\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3358 - accuracy: 0.9310 - precision_12: 0.9101 - recall_12: 0.9463 - auc_12: 0.9783\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.3483 - accuracy: 0.9321 - precision_12: 0.9841 - recall_12: 0.8692 - auc_12: 0.9724\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3055 - accuracy: 0.9485 - precision_12: 0.9339 - recall_12: 0.9579 - auc_12: 0.9814\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.2971 - accuracy: 0.9507 - precision_12: 0.9614 - recall_12: 0.9322 - auc_12: 0.9762\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.2956 - accuracy: 0.9463 - precision_12: 0.9317 - recall_12: 0.9556 - auc_12: 0.9811\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.2653 - accuracy: 0.9540 - precision_12: 0.9684 - recall_12: 0.9322 - auc_12: 0.9848\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2458 - accuracy: 0.9693 - precision_12: 0.9695 - recall_12: 0.9650 - auc_12: 0.9850\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2458 - accuracy: 0.9606 - precision_12: 0.9667 - recall_12: 0.9486 - auc_12: 0.9866\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2235 - accuracy: 0.9617 - precision_12: 0.9712 - recall_12: 0.9463 - auc_12: 0.9914\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f71249c8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.5682 - accuracy: 0.5115 - precision_13: 0.4784 - recall_13: 0.4650 - auc_13: 0.5093\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.4624 - accuracy: 0.4995 - precision_13: 0.4619 - recall_13: 0.4112 - auc_13: 0.5153\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.3390 - accuracy: 0.5553 - precision_13: 0.5291 - recall_13: 0.4673 - auc_13: 0.5815\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 1.3365 - accuracy: 0.5279 - precision_13: 0.4962 - recall_13: 0.4603 - auc_13: 0.5410\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.2541 - accuracy: 0.5717 - precision_13: 0.5499 - recall_13: 0.4766 - auc_13: 0.6118\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.2258 - accuracy: 0.6079 - precision_13: 0.5875 - recall_13: 0.5491 - auc_13: 0.6401\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.1529 - accuracy: 0.6407 - precision_13: 0.6064 - recall_13: 0.6659 - auc_13: 0.6974\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 1.1220 - accuracy: 0.6298 - precision_13: 0.6197 - recall_13: 0.5444 - auc_13: 0.7090\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.1220 - accuracy: 0.6725 - precision_13: 0.6393 - recall_13: 0.6916 - auc_13: 0.7267\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.9879 - accuracy: 0.7054 - precision_13: 0.7166 - recall_13: 0.6145 - auc_13: 0.8091\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.9046 - accuracy: 0.7777 - precision_13: 0.8223 - recall_13: 0.6706 - auc_13: 0.8690\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.8706 - accuracy: 0.7985 - precision_13: 0.7563 - recall_13: 0.8411 - auc_13: 0.8758\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.8272 - accuracy: 0.8105 - precision_13: 0.7891 - recall_13: 0.8131 - auc_13: 0.8888\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.7279 - accuracy: 0.8576 - precision_13: 0.8531 - recall_13: 0.8411 - auc_13: 0.9308\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.7019 - accuracy: 0.8719 - precision_13: 0.8388 - recall_13: 0.8995 - auc_13: 0.9398\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.6487 - accuracy: 0.8773 - precision_13: 0.8970 - recall_13: 0.8341 - auc_13: 0.9519\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.6199 - accuracy: 0.8839 - precision_13: 0.8889 - recall_13: 0.8598 - auc_13: 0.9576\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5946 - accuracy: 0.9025 - precision_13: 0.9065 - recall_13: 0.8832 - auc_13: 0.9597\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.6080 - accuracy: 0.8740 - precision_13: 0.8309 - recall_13: 0.9182 - auc_13: 0.9555\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5760 - accuracy: 0.8970 - precision_13: 0.8761 - recall_13: 0.9089 - auc_13: 0.9594\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5348 - accuracy: 0.9113 - precision_13: 0.9370 - recall_13: 0.8692 - auc_13: 0.9685\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.5204 - accuracy: 0.9146 - precision_13: 0.9167 - recall_13: 0.8995 - auc_13: 0.9673\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4455 - accuracy: 0.9430 - precision_13: 0.9352 - recall_13: 0.9439 - auc_13: 0.9823\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.4291 - accuracy: 0.9277 - precision_13: 0.9436 - recall_13: 0.8995 - auc_13: 0.9850\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4070 - accuracy: 0.9430 - precision_13: 0.9196 - recall_13: 0.9626 - auc_13: 0.9881\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.3958 - accuracy: 0.9485 - precision_13: 0.9547 - recall_13: 0.9346 - auc_13: 0.9866\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.3674 - accuracy: 0.9551 - precision_13: 0.9490 - recall_13: 0.9556 - auc_13: 0.9900\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3865 - accuracy: 0.9409 - precision_13: 0.9212 - recall_13: 0.9556 - auc_13: 0.9864\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3917 - accuracy: 0.9288 - precision_13: 0.9642 - recall_13: 0.8808 - auc_13: 0.9861\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3554 - accuracy: 0.9441 - precision_13: 0.9274 - recall_13: 0.9556 - auc_13: 0.9886\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3626 - accuracy: 0.9507 - precision_13: 0.9443 - recall_13: 0.9509 - auc_13: 0.9840 0s - loss: 0.3618 - accuracy: 0.9516 - precision_13: 0.9450 - recall_13: 0.9542 - auc_13: \n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.3843 - accuracy: 0.9332 - precision_13: 0.9622 - recall_13: 0.8925 - auc_13: 0.9816\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3512 - accuracy: 0.9441 - precision_13: 0.9180 - recall_13: 0.9673 - auc_13: 0.9857\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3905 - accuracy: 0.9200 - precision_13: 0.9404 - recall_13: 0.8855 - auc_13: 0.9756\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3303 - accuracy: 0.9452 - precision_13: 0.9335 - recall_13: 0.9509 - auc_13: 0.9847\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2880 - accuracy: 0.9639 - precision_13: 0.9759 - recall_13: 0.9463 - auc_13: 0.9910 0s - loss: 0.2896 - accuracy: 0.9632 - precision_13: 0.9755 - recall_13: 0.9454 - auc_13: 0.99\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2732 - accuracy: 0.9639 - precision_13: 0.9478 - recall_13: 0.9766 - auc_13: 0.9931\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2680 - accuracy: 0.9639 - precision_13: 0.9647 - recall_13: 0.9579 - auc_13: 0.9914\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2451 - accuracy: 0.9693 - precision_13: 0.9651 - recall_13: 0.9696 - auc_13: 0.9951\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2585 - accuracy: 0.9617 - precision_13: 0.9602 - recall_13: 0.9579 - auc_13: 0.9915\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.2375 - accuracy: 0.9682 - precision_13: 0.9672 - recall_13: 0.9650 - auc_13: 0.9938\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.2446 - accuracy: 0.9639 - precision_13: 0.9714 - recall_13: 0.9509 - auc_13: 0.9917\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2323 - accuracy: 0.9660 - precision_13: 0.9737 - recall_13: 0.9533 - auc_13: 0.9928\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.2088 - accuracy: 0.9726 - precision_13: 0.9632 - recall_13: 0.9790 - auc_13: 0.9955\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1922 - accuracy: 0.9748 - precision_13: 0.9856 - recall_13: 0.9603 - auc_13: 0.9972\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.2183 - accuracy: 0.9562 - precision_13: 0.9389 - recall_13: 0.9696 - auc_13: 0.9934\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.2721 - accuracy: 0.9430 - precision_13: 0.9393 - recall_13: 0.9393 - auc_13: 0.9844\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2747 - accuracy: 0.9321 - precision_13: 0.9236 - recall_13: 0.9322 - auc_13: 0.9831\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2210 - accuracy: 0.9606 - precision_13: 0.9414 - recall_13: 0.9766 - auc_13: 0.9923\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1864 - accuracy: 0.9682 - precision_13: 0.9650 - recall_13: 0.9673 - auc_13: 0.9958\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7124ecd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 1.5230 - accuracy: 0.5159 - precision_14: 0.4850 - recall_14: 0.5280 - auc_14: 0.5112\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 1.4376 - accuracy: 0.5268 - precision_14: 0.4935 - recall_14: 0.3528 - auc_14: 0.5105\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.3750 - accuracy: 0.5082 - precision_14: 0.4746 - recall_14: 0.4579 - auc_14: 0.5069\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 1.3288 - accuracy: 0.5115 - precision_14: 0.4790 - recall_14: 0.4790 - auc_14: 0.5165\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 1.2915 - accuracy: 0.5312 - precision_14: 0.5000 - recall_14: 0.3598 - auc_14: 0.5186\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.2395 - accuracy: 0.5334 - precision_14: 0.5024 - recall_14: 0.4836 - auc_14: 0.5488\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.2176 - accuracy: 0.5257 - precision_14: 0.4943 - recall_14: 0.5023 - auc_14: 0.5417\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 1.1968 - accuracy: 0.5444 - precision_14: 0.5184 - recall_14: 0.3949 - auc_14: 0.5558\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.1346 - accuracy: 0.5761 - precision_14: 0.5451 - recall_14: 0.5794 - auc_14: 0.6122\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 1.1548 - accuracy: 0.5214 - precision_14: 0.4862 - recall_14: 0.3692 - auc_14: 0.5226\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 1.1346 - accuracy: 0.5148 - precision_14: 0.4835 - recall_14: 0.5140 - auc_14: 0.5196\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.1012 - accuracy: 0.5301 - precision_14: 0.4987 - recall_14: 0.4463 - auc_14: 0.5408\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 1.0904 - accuracy: 0.5181 - precision_14: 0.4808 - recall_14: 0.3505 - auc_14: 0.5256\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 1.0664 - accuracy: 0.5312 - precision_14: 0.5000 - recall_14: 0.4322 - auc_14: 0.5485\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.0376 - accuracy: 0.5389 - precision_14: 0.5085 - recall_14: 0.4883 - auc_14: 0.5684\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 1.0324 - accuracy: 0.5279 - precision_14: 0.4956 - recall_14: 0.3925 - auc_14: 0.5433\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 1.0297 - accuracy: 0.5225 - precision_14: 0.4910 - recall_14: 0.5093 - auc_14: 0.5350\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.9965 - accuracy: 0.5531 - precision_14: 0.5281 - recall_14: 0.4393 - auc_14: 0.5832\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 1.0197 - accuracy: 0.5159 - precision_14: 0.4836 - recall_14: 0.4813 - auc_14: 0.5361\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.9802 - accuracy: 0.5225 - precision_14: 0.4916 - recall_14: 0.5491 - auc_14: 0.5456\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.9804 - accuracy: 0.5257 - precision_14: 0.4889 - recall_14: 0.2570 - auc_14: 0.5391\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.9344 - accuracy: 0.5586 - precision_14: 0.5293 - recall_14: 0.5280 - auc_14: 0.5829\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.9365 - accuracy: 0.5378 - precision_14: 0.5074 - recall_14: 0.4836 - auc_14: 0.5707\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.9303 - accuracy: 0.5465 - precision_14: 0.5240 - recall_14: 0.3575 - auc_14: 0.5513\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.9304 - accuracy: 0.5027 - precision_14: 0.4661 - recall_14: 0.4182 - auc_14: 0.5199\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.9280 - accuracy: 0.4995 - precision_14: 0.4712 - recall_14: 0.5537 - auc_14: 0.5153\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.9125 - accuracy: 0.4940 - precision_14: 0.4581 - recall_14: 0.4346 - auc_14: 0.5127\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.8941 - accuracy: 0.5345 - precision_14: 0.5045 - recall_14: 0.3949 - auc_14: 0.5573\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.8685 - accuracy: 0.5367 - precision_14: 0.5059 - recall_14: 0.5000 - auc_14: 0.5800\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.8698 - accuracy: 0.5619 - precision_14: 0.5343 - recall_14: 0.5093 - auc_14: 0.5780\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.8470 - accuracy: 0.5465 - precision_14: 0.5156 - recall_14: 0.5397 - auc_14: 0.5929\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.8338 - accuracy: 0.5685 - precision_14: 0.5421 - recall_14: 0.5117 - auc_14: 0.6202\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.8194 - accuracy: 0.5915 - precision_14: 0.5551 - recall_14: 0.6472 - auc_14: 0.6335\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.8070 - accuracy: 0.5882 - precision_14: 0.5580 - recall_14: 0.5841 - auc_14: 0.6495\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.7914 - accuracy: 0.5882 - precision_14: 0.5599 - recall_14: 0.5678 - auc_14: 0.6531\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.7915 - accuracy: 0.6112 - precision_14: 0.5765 - recall_14: 0.6425 - auc_14: 0.6506\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.8106 - accuracy: 0.6298 - precision_14: 0.5849 - recall_14: 0.7243 - auc_14: 0.6606\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.7953 - accuracy: 0.5882 - precision_14: 0.5395 - recall_14: 0.8294 - auc_14: 0.6702\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.7396 - accuracy: 0.6627 - precision_14: 0.6676 - recall_14: 0.5584 - auc_14: 0.7289\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.7216 - accuracy: 0.6616 - precision_14: 0.6842 - recall_14: 0.5164 - auc_14: 0.7317\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.6412 - accuracy: 0.7251 - precision_14: 0.6863 - recall_14: 0.7617 - auc_14: 0.8299\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.6292 - accuracy: 0.7470 - precision_14: 0.7373 - recall_14: 0.7150 - auc_14: 0.8304\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.6349 - accuracy: 0.7514 - precision_14: 0.7799 - recall_14: 0.6542 - auc_14: 0.8197\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.6102 - accuracy: 0.7568 - precision_14: 0.8179 - recall_14: 0.6192 - auc_14: 0.8358\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.5693 - accuracy: 0.7579 - precision_14: 0.7518 - recall_14: 0.7220 - auc_14: 0.8540\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.6047 - accuracy: 0.7459 - precision_14: 0.7438 - recall_14: 0.6986 - auc_14: 0.8374\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.5813 - accuracy: 0.7503 - precision_14: 0.7809 - recall_14: 0.6495 - auc_14: 0.8369\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.5532 - accuracy: 0.7722 - precision_14: 0.8125 - recall_14: 0.6682 - auc_14: 0.8566\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.5760 - accuracy: 0.7744 - precision_14: 0.7921 - recall_14: 0.7033 - auc_14: 0.8472\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.5586 - accuracy: 0.7744 - precision_14: 0.8854 - recall_14: 0.5958 - auc_14: 0.8571\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7125d79598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 85.939% (+/-11.355) \n",
      " Precision: 85.646% (+/-17.025) \n",
      " Recall: 78.409% (+/-10.365) \n",
      " AUC: 92.315% (+/-8.070) \n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_3(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 1, 50, 128\n",
    "    n_features = trainX.shape[2]\n",
    "    n_steps, n_length = 10, 625\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='selu'),\n",
    "    input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='selu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='selu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(50, activation= 'selu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01) ))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation= 'sigmoid' ))\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=['accuracy',Precision(),Recall(),AUC()])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy,precision,recall,auc = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy,precision,recall,auc\n",
    "\n",
    "def run_experiment_3(X,y,repeats=5):\n",
    "    # load data\n",
    "    trainX, testX,trainy, testy = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    # repeat experiment\n",
    "    accuracies = list()\n",
    "    precisions = list()\n",
    "    recalls = list()\n",
    "    aucs = list()\n",
    "    for r in range(repeats):\n",
    "        accuracy,precision,recall,auc = evaluate_model_3(trainX, trainy, testX, testy)\n",
    "        accuracy = accuracy * 100.0\n",
    "        precision = precision * 100.0\n",
    "        recall = recall * 100.0\n",
    "        auc = auc * 100.0\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        aucs.append(auc)\n",
    "    # summarize results\n",
    "    summarize_results(accuracies,precisions,recalls,aucs)\n",
    "    \n",
    "run_experiment_3(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
